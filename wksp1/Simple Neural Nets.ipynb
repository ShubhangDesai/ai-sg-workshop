{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Neural Nets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to create a neural network that is trained on dummy data. Here's is our data below. Each training example's features are either 1 or 0, and you'll notice that the label is always exactly equal to the first feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([[1, 1, 0, 1], [0, 1, 0, 1], [0, 1, 0, 1], [1, 0, 1, 0],\n",
    "              [0, 1, 1, 0], [1, 0, 1, 1], [0, 0, 0, 0], [1, 1, 1, 0],\n",
    "              [0, 0, 1, 1], [1, 1, 0, 1], [0, 0, 1, 0], [1, 0, 0, 0],\n",
    "              [1, 1, 1, 1], [0, 1, 1, 1], [1, 0, 0, 1], [1, 0, 0, 1]])\n",
    "y = np.array([[0], [0], [0], [1], [1], [1], [0], [1], [1], [0], [1], [0], [1], [1], [0], [0]])\n",
    "\n",
    "print(X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's split this up into the train, validation, and test datasets. Make it a 50:25:25 split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = None\n",
    "X_val = None\n",
    "X_test = None\n",
    "\n",
    "y_train = None\n",
    "y_val = None\n",
    "y_test = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward Pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to create a simple 1-layer fully-connected neural network, as you practiced in the previous notebook. A fully connected notebook consists of a multiplicative weight matrix and an additive bias vector. Intialize the weight and bias to random values. Make sure that the shapes of the two are correct to transform each training example from having 4 elements (for each feature) to 2 (one for each label i.e. what the network predicts the answer to be)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = None\n",
    "b = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's create a function that acts as the linear layer for the network. Don't worry about the 'linear_cache' variable for now, we'll discuss that later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_cache = {}\n",
    "def linear(input):\n",
    "    output = None # Calculate the output\n",
    "    linear_cache[\"input\"] = input\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The linear layer will output a vector with 2 elements. The 0th element will contain a 1 if the network thinks the correct label is 0, and the 1st element will contain a 1 if the network thinks the correct label is 1 (this is called one-hot encoding).\n",
    "\n",
    "This is great, but the output will be arbitrarily scaled. We want to turn the output into a probability distribution. So, let's implement a softmax function below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "softmax_cache = {}\n",
    "def softmax_cross_entropy(input, y):\n",
    "    batch_size = input.shape[1]\n",
    "    indeces = np.arange(batch_size)\n",
    " \n",
    "    exp = None # Exponentiate the whole input\n",
    "    prob = None # Normalize the exp to give the probability distribution\n",
    "    \n",
    "    losses = None # Find loss value of each example\n",
    "    loss = None # Calculate total empirical loss\n",
    "    \n",
    "    softmax_cache[\"prob\"], softmax_cache[\"y\"], softmax_cache[\"indeces\"] = prob, y, indeces\n",
    "    return prob, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_pass(input, y):\n",
    "    linear_output = linear(input)\n",
    "    prob, loss = softmax_cross_entropy(linear_output, y)\n",
    "    return prob, loss\n",
    "\n",
    "forward_pass(X_val, y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backward Pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we discussed during the lecture, implement the backward pass for the softmax and linear layers. Modify your above code to include variables in the caches as necessary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_cross_entropy_backward():\n",
    "    prob, y, indeces = softmax_cache[\"prob\"], softmax_cache[\"y\"], softmax_cache[\"indeces\"]\n",
    "    dloss = None # Calculate gradient of the loss\n",
    "    return dloss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_backward(dout):\n",
    "    input = linear_cache[\"input\"]\n",
    "    dW = None # Calculate gradient of weight matrix\n",
    "    db = None # Calculate gradient of bias vector\n",
    "    return dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_pass():\n",
    "    dloss = softmax_cross_entropy_backward()\n",
    "    dW, db = linear_backward(dloss)\n",
    "    return dW, db\n",
    "\n",
    "backward_pass()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's train our network! We provided below a useful function to test the accuracy of your network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_accuracy(output, target):\n",
    "    pred = np.argmax(output, axis=1)\n",
    "    target = np.reshape(target, (target.shape[0]))\n",
    "    correct = np.sum(pred == target)\n",
    "    accuracy = correct / pred.shape[0] * 100\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now create your training regime which 1) samples a batch of training data 2) passes the batch forward through the network and computes loss 3) backpropigates into the weight and bias 4) updates the weights and 5) periodically outputs the accuracy on the training and validation sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(4000):\n",
    "    indeces = np.random.choice(X_train.shape[0], 4)\n",
    "    batch = X_train[indeces, :]\n",
    "    target = y_train[indeces]\n",
    "\n",
    "    # Forward Pass\n",
    "    prob, losses = forward_pass(batch, target)\n",
    "\n",
    "    # Backward Pass\n",
    "    dW, db = backward_pass()\n",
    "\n",
    "    # TODO: Weight updates\n",
    "\n",
    "    # Evaluation\n",
    "    if (i+1) % 100 == 0:\n",
    "        accuracy = None # Calculate training accuracy\n",
    "        print (\"Training Accuracy: %f\" % accuracy)\n",
    "\n",
    "    if (i+1) % 500 == 0:\n",
    "        accuracy = None # Calculate validation accuracy\n",
    "        print(\"Validation Accuracy: %f\\n\" % accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, check your accuracy on the test set to see how you did!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = None # Calculate test accuracy\n",
    "print(\"Test Accuracy: %f\" % accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
